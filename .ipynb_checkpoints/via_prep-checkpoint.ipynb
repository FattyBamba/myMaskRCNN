{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(dataset_dir, src_file, dst_file='/tmp/annotations.json', validate=True):\n",
    "    \"\"\"\n",
    "    Scans a VIA generated annotations file and inserts height and width.\n",
    "    :param dataset_dir: Directory containing the data set and annotation file.\n",
    "    :param src_file: Annotation file name.\n",
    "    :param dst_file: File to write the updates to.\n",
    "    :param validate: If trued runs validation checks on the annotation file.\n",
    "    \"\"\"\n",
    "\n",
    "    annotations_filepath = os.path.join(dataset_dir, src_file)\n",
    "\n",
    "    with open(annotations_filepath, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    for k, a in annotations.items():\n",
    "\n",
    "        # Set shape so we don't have during training.\n",
    "        image_path = os.path.join(dataset_dir, a['filename'])\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(\"Unable to read: \", image_path)\n",
    "\n",
    "        print(image_path, type(image))\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        a['height'] = height\n",
    "        a['width'] = width\n",
    "\n",
    "        # Perform some sanity checking and corrections.\n",
    "        if validate:\n",
    "            # Ensure polygon points fall within the image shape\n",
    "            for r_idx, r in a['regions'].items():\n",
    "\n",
    "                # Make sure x points are in bound\n",
    "                all_points_x = r['shape_attributes']['all_points_x']\n",
    "                for x_idx, x in enumerate(all_points_x):\n",
    "                    if x > width:\n",
    "                        print('Correcting all_points_x for ', a['filename'])\n",
    "                        all_points_x[x_idx] = width - 1\n",
    "\n",
    "                # Make sure y points are in bound\n",
    "                all_points_y = r['shape_attributes']['all_points_y']\n",
    "                for y_idx, y in enumerate(all_points_y):\n",
    "                    if y > height:\n",
    "                        print('Correcting all_points_x for ', a['filename'])\n",
    "                        all_points_y[y_idx] = height - 1\n",
    "\n",
    "    with open(dst_file, 'w') as f:\n",
    "        json.dump(annotations, f)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    import argparse\n",
    "\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Pre-processing via annotation file.')\n",
    "    parser.add_argument('--dataset', required=True,\n",
    "                        metavar=\"/path/to/dataset/\",\n",
    "                        help='Directory of the dataset')\n",
    "    parser.add_argument('--annotations', required=True,\n",
    "                        metavar=\"annotations.json\",\n",
    "                        default=\"annotations.json\",\n",
    "                        help=\"annotations file name\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"Annotations: \", args.annotations)\n",
    "    print(\"Dataset: \", args.dataset)\n",
    "\n",
    "    preprocess(args.dataset, args.annotations)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
